{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hqHmZLK_DR4"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_w6ZgQUkxqS",
        "outputId": "895b1a44-a4b0-461f-cbb3-28bcffdb7c2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m71.7/73.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.244-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.13-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.11 (from langchain)\n",
            "  Downloading langsmith-0.0.14-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.11)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, langsmith, openai, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.13 langchain-0.0.244 langsmith-0.0.14 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 python-dotenv-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai python-dotenv langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pf9-R2SLk7a2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import dotenv\n",
        "\n",
        "#from dotenv import load_dotenv, find_dotenvo\n",
        "_ = dotenv.load_dotenv(dotenv.find_dotenv()) # read local .env file\n",
        "openai.api_key = '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' #os.environ['OPENAI_API_KEY']\n",
        "os.environ['OPENAI_API_KEY'] = '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'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBFnbJcKtRul"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RxaL6oJQwLa2",
        "outputId": "cc522863-6ff1-4c31-d192-c43585d4a204"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I'm sorry, but I don't understand what you're trying to say. Can you please provide more information or clarify your question?\""
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_completion('kasjhdg riop where op')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_lqVhOp1CwZ"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMb44vwy1PVl"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(temperature=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8hHj1erP15E0",
        "outputId": "ea277633-8c2a-46e6-c4dc-7b0a8f0c8dfd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tell me how {entity1} would {action} the {quality} of {entity2} in 2 sentences in the words of {entity1}'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "template = '''Tell me how {entity1} would {action} the {quality} of {entity2} in 2 sentences in the words of {entity1}'''\n",
        "template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr_O-Uo422qT",
        "outputId": "b6ffe6df-5f32-471d-c0d2-b0705fd8b9d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['entity1', 'entity2', 'action', 'quality'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['action', 'entity1', 'entity2', 'quality'], output_parser=None, partial_variables={}, template='Tell me how {entity1} would {action} the {quality} of {entity2} in 2 sentences in the words of {entity1}', template_format='f-string', validate_template=True), additional_kwargs={})])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4rL6ktF3Sa5",
        "outputId": "fdf998fe-c183-4712-f301-64198f4399f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Tell me how Nasa would feel about the launches of Spacex in 2 sentences in the words of Nasa',\n",
              " \"NASA is thrilled to witness the successful launches of SpaceX, as it represents a significant step towards advancing space exploration and fostering collaboration within the industry. We admire SpaceX's innovation and look forward to continued partnership in our shared mission of pushing the boundaries of human spaceflight.\")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "entity1 = 'marslings'\n",
        "entity2 = 'chatgpt'\n",
        "quality = 'power'\n",
        "action = 'view'\n",
        "\n",
        "entity1 = 'Nasa'\n",
        "entity2 = 'Spacex'\n",
        "quality = 'launches'\n",
        "action = 'feel about'\n",
        "\n",
        "inp = prompt.format_messages(\n",
        "                    entity1=entity1,\n",
        "                    entity2=entity2,\n",
        "                    quality=quality,\n",
        "                    action=action)\n",
        "out = chat(inp)\n",
        "inp[0].content, out.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZiKGC8JziZx"
      },
      "source": [
        "## Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhxC8HIo5vyb"
      },
      "outputs": [],
      "source": [
        "# response schema for dict keys\n",
        "# StructuredOutputParser\n",
        "from langchain.output_parsers import StructuredOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6oGkjvv5-9U"
      },
      "source": [
        "## Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWOYKXK55-aK"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXcTqKD56w0z"
      },
      "outputs": [],
      "source": [
        "model = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzOwsDH45-QQ"
      },
      "outputs": [],
      "source": [
        "memory = ConversationSummaryMemory(llm=model, max_token_limit=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2j1SuJks8rFg"
      },
      "outputs": [],
      "source": [
        "memory.save_context({'input':'hey'}, {'output':'yo'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVhgx9Jf_mxG",
        "outputId": "7d801aef-2dd4-41cd-f8a6-d3b2c2bd7a05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': 'The human greets the AI with \"hey\" and the AI responds with \"yo\".'}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgdWBJaR7Ibr"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(llm=model, memory=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhFXOhczDCwS"
      },
      "outputs": [],
      "source": [
        "memory.buffer = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_UjpnbdADRu",
        "outputId": "d2d7cf24-ec61-4921-ecb9-1848497df82d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-gV2pYpCXGz7sP9a8WBCm2S9Q on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-gV2pYpCXGz7sP9a8WBCm2S9Q on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-gV2pYpCXGz7sP9a8WBCm2S9Q on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-gV2pYpCXGz7sP9a8WBCm2S9Q on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-gV2pYpCXGz7sP9a8WBCm2S9Q on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'what is openai',\n",
              " 'history': 'The human greets the AI and the AI, named OpenAI, asks how it can assist.',\n",
              " 'response': \"OpenAI is an artificial intelligence research laboratory and company. It was founded in December 2015 by Elon Musk, Sam Altman, Greg Brockman, Ilya Sutskever, John Schulman, and Wojciech Zaremba. OpenAI's mission is to ensure that artificial general intelligence (AGI) benefits all of humanity. AGI refers to highly autonomous systems that outperform humans at most economically valuable work. OpenAI conducts research in various areas of AI and also develops and deploys AI models and systems. Is there anything specific you would like to know about OpenAI?\"}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain('what is openai')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg7wuNnneltO"
      },
      "source": [
        "# chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxo4IWpten4z"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain, SequentialChain, SimpleSequentialChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0)\n",
        "prompt1 = ChatPromptTemplate.from_template(''' if {inp} is a celebrity, then tell the user the dob of the person,only the dob not a word more, else return the keyword 'none' ''')\n",
        "prompt2 = ChatPromptTemplate.from_template(''' if {inp} is an event, describe the event, else return none''')\n",
        "prompt3 = ChatPromptTemplate.from_template(''' if {dob} is none, tell me something funny else list two celebrities with dob same as {dob}''')\n",
        "prompt4 = ChatPromptTemplate.from_template(''' output the person responsible for the {event}''')\n",
        "\n",
        "#chain1 = LLMChain(llm=llm, prompt='''you are a someone who knows everything and answers to users questions. Ask the user for input{sth}''', output_keys=about)\n",
        "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key='dob')\n",
        "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key='event')\n",
        "\n",
        "chain3 = LLMChain(llm=llm, prompt=prompt3, output_key='celeb')\n",
        "chain4 = LLMChain(llm=llm, prompt=prompt4, output_key='resp')\n",
        "chain = SequentialChain(chains=[chain1,chain2,chain3,chain4], input_variables=[\"inp\"],\n",
        "    output_variables=[\"dob\", \"event\",\"celeb\", \"resp\"],\n",
        "    verbose=True)"
      ],
      "metadata": {
        "id": "gIuu38L_981F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain('margot robbie')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNmz5Fk3DU1a",
        "outputId": "3ea756fd-1585-4329-9561-19091db315f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-gV2pYpCXGz7sP9a8WBCm2S9Q on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-gV2pYpCXGz7sP9a8WBCm2S9Q on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-gV2pYpCXGz7sP9a8WBCm2S9Q on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-gV2pYpCXGz7sP9a8WBCm2S9Q on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'inp': 'margot robbie',\n",
              " 'dob': 'none',\n",
              " 'event': 'None',\n",
              " 'celeb': 'None is a special value in programming that represents the absence of a value. It does not have a date of birth (DOB) as it is not a person or entity. Therefore, there are no celebrities with a DOB same as \"none.\"\\n\\nHowever, here\\'s a funny joke for you:\\n\\nWhy don\\'t scientists trust atoms?\\n\\nBecause they make up everything!',\n",
              " 'resp': 'I\\'m sorry, but I cannot provide an output for the person responsible for \"None\" as it is not a specific entity or individual.'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl7uSbg17UmB"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "langchain\n",
        "  - chat_models\n",
        "    - chatopenai\n",
        "\n",
        "  - prompts\n",
        "    - chatprompttemplate.from_template\n",
        "    - .format_messages\n",
        "\n",
        "  - parsers\n",
        "    - response schema for dict keys\n",
        "    - StructuredOutputParser.from_response_schemas\n",
        "    - get_format_instructions - to format prompt accordingly\n",
        "    - .parse()\n",
        "\n",
        "  - memory\n",
        "    - conv buffer mem - stores context with no limit\n",
        "    - conv buffer window mem - stores a window of context, n exchanges say\n",
        "    - conv token buffer memory - stores limited tokens\n",
        "    - conv summary memory - summarizes history and stores context to max_limit tokens\n",
        "\n",
        "  - chains\n",
        "    - conv chain\n",
        "    - LLMChain\n",
        "    - SimpleSequentialChain\n",
        "    - SequentialChain\n",
        "    - router\n",
        "      - MultiPromptChain\n",
        "      - llm_router - LLMRouterChain,RouterOutputParser\n",
        "\n",
        "  - agents\n",
        "    -\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# llama 2"
      ],
      "metadata": {
        "id": "xuD66Pmc_ox8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/llama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YOA6THi_qLR",
        "outputId": "e8aa2d51-235c-4f79-d875-b7e3c4aa4f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Total 91 (delta 0), reused 0 (delta 0), pack-reused 91\u001b[K\n",
            "Receiving objects: 100% (91/91), 1017.24 KiB | 31.79 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQfDyUuG_0Hk",
        "outputId": "d2e95b00-89ab-4353-e77c-5ea3dc8e2f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "https://download.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiXHUwMDE1Kz8rP01fPyIsIlJlc291cmNlIjoiaHR0cHM6XC9cL2Rvd25sb2FkLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTA1NDIwMDd9fX1dfQ__&Signature=qUhzdbuQdJXmV63StYpm5YMpsZRI6Srz5-TL-uM9lE1JhBCy7G9JuW-TXAJi52CHDlgFQ3j-uwbuPBsQ8--kOcSXbXQc7Bt9lBv0lBunFp%7EmpGvVaa%7E%7Eop%7Exs45HIHpkvY2%7EmJoEI-NHOs51t0B57-C7Gi1qOCT3KsHM1gwaRk7ykcCOc2K%7E2yQvLWt2wxHBAQnFz7WNuuRc%7E8sIWAcrutksLa5H2%7EtJ6OBSCCusKMWUvh9Hi41oi-FasKGhzZ5VlnVJQfOSLJ5YjhdSmfc5qIsRg-LvYyCsabY43v0DursX2Tl01%7EpqkssDlfq6hrhg468-xK%7E-cla-f97%7E%7E6jf6w__&Key-Pair-Id=K15QRJLYKIFSLZ\n",
        "'''"
      ],
      "metadata": {
        "id": "jB6tvUUhB5l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 download.sh"
      ],
      "metadata": {
        "id": "57y47A0BCsNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./download.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWcal3aiCS8S",
        "outputId": "3890b9b7-404b-4357-d912-6c94612ab9e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the URL from email: https://download.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiXHUwMDE1Kz8rP01fPyIsIlJlc291cmNlIjoiaHR0cHM6XC9cL2Rvd25sb2FkLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTA1NDIwMDd9fX1dfQ__&Signature=qUhzdbuQdJXmV63StYpm5YMpsZRI6Srz5-TL-uM9lE1JhBCy7G9JuW-TXAJi52CHDlgFQ3j-uwbuPBsQ8--kOcSXbXQc7Bt9lBv0lBunFp%7EmpGvVaa%7E%7Eop%7Exs45HIHpkvY2%7EmJoEI-NHOs51t0B57-C7Gi1qOCT3KsHM1gwaRk7ykcCOc2K%7E2yQvLWt2wxHBAQnFz7WNuuRc%7E8sIWAcrutksLa5H2%7EtJ6OBSCCusKMWUvh9Hi41oi-FasKGhzZ5VlnVJQfOSLJ5YjhdSmfc5qIsRg-LvYyCsabY43v0DursX2Tl01%7EpqkssDlfq6hrhg468-xK%7E-cla-f97%7E%7E6jf6w__&Key-Pair-Id=K15QRJLYKIFSLZ\n",
            "\n",
            "Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 7B-chat\n",
            "Downloading LICENSE and Acceptable Usage Policy\n",
            "--2023-07-27 11:20:56--  https://download.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiXHUwMDE1Kz8rP01fPyIsIlJlc291cmNlIjoiaHR0cHM6XC9cL2Rvd25sb2FkLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTA1NDIwMDd9fX1dfQ__&Signature=qUhzdbuQdJXmV63StYpm5YMpsZRI6Srz5-TL-uM9lE1JhBCy7G9JuW-TXAJi52CHDlgFQ3j-uwbuPBsQ8--kOcSXbXQc7Bt9lBv0lBunFp%7EmpGvVaa%7E%7Eop%7Exs45HIHpkvY2%7EmJoEI-NHOs51t0B57-C7Gi1qOCT3KsHM1gwaRk7ykcCOc2K%7E2yQvLWt2wxHBAQnFz7WNuuRc%7E8sIWAcrutksLa5H2%7EtJ6OBSCCusKMWUvh9Hi41oi-FasKGhzZ5VlnVJQfOSLJ5YjhdSmfc5qIsRg-LvYyCsabY43v0DursX2Tl01%7EpqkssDlfq6hrhg468-xK%7E-cla-f97%7E%7E6jf6w__&Key-Pair-Id=K15QRJLYKIFSLZ\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 13.33.88.72, 13.33.88.45, 13.33.88.62, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|13.33.88.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7020 (6.9K) [binary/octet-stream]\n",
            "Saving to: ‘./LICENSE’\n",
            "\n",
            "./LICENSE           100%[===================>]   6.86K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-27 11:20:56 (337 MB/s) - ‘./LICENSE’ saved [7020/7020]\n",
            "\n",
            "--2023-07-27 11:20:56--  https://download.llamameta.net/USE_POLICY.md?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiXHUwMDE1Kz8rP01fPyIsIlJlc291cmNlIjoiaHR0cHM6XC9cL2Rvd25sb2FkLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTA1NDIwMDd9fX1dfQ__&Signature=qUhzdbuQdJXmV63StYpm5YMpsZRI6Srz5-TL-uM9lE1JhBCy7G9JuW-TXAJi52CHDlgFQ3j-uwbuPBsQ8--kOcSXbXQc7Bt9lBv0lBunFp%7EmpGvVaa%7E%7Eop%7Exs45HIHpkvY2%7EmJoEI-NHOs51t0B57-C7Gi1qOCT3KsHM1gwaRk7ykcCOc2K%7E2yQvLWt2wxHBAQnFz7WNuuRc%7E8sIWAcrutksLa5H2%7EtJ6OBSCCusKMWUvh9Hi41oi-FasKGhzZ5VlnVJQfOSLJ5YjhdSmfc5qIsRg-LvYyCsabY43v0DursX2Tl01%7EpqkssDlfq6hrhg468-xK%7E-cla-f97%7E%7E6jf6w__&Key-Pair-Id=K15QRJLYKIFSLZ\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 13.33.88.72, 13.33.88.45, 13.33.88.62, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|13.33.88.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4766 (4.7K) [binary/octet-stream]\n",
            "Saving to: ‘./USE_POLICY.md’\n",
            "\n",
            "./USE_POLICY.md     100%[===================>]   4.65K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-27 11:20:56 (10.5 MB/s) - ‘./USE_POLICY.md’ saved [4766/4766]\n",
            "\n",
            "Downloading tokenizer\n",
            "--2023-07-27 11:20:56--  https://download.llamameta.net/tokenizer.model?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiXHUwMDE1Kz8rP01fPyIsIlJlc291cmNlIjoiaHR0cHM6XC9cL2Rvd25sb2FkLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTA1NDIwMDd9fX1dfQ__&Signature=qUhzdbuQdJXmV63StYpm5YMpsZRI6Srz5-TL-uM9lE1JhBCy7G9JuW-TXAJi52CHDlgFQ3j-uwbuPBsQ8--kOcSXbXQc7Bt9lBv0lBunFp%7EmpGvVaa%7E%7Eop%7Exs45HIHpkvY2%7EmJoEI-NHOs51t0B57-C7Gi1qOCT3KsHM1gwaRk7ykcCOc2K%7E2yQvLWt2wxHBAQnFz7WNuuRc%7E8sIWAcrutksLa5H2%7EtJ6OBSCCusKMWUvh9Hi41oi-FasKGhzZ5VlnVJQfOSLJ5YjhdSmfc5qIsRg-LvYyCsabY43v0DursX2Tl01%7EpqkssDlfq6hrhg468-xK%7E-cla-f97%7E%7E6jf6w__&Key-Pair-Id=K15QRJLYKIFSLZ\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 13.33.88.72, 13.33.88.45, 13.33.88.62, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|13.33.88.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 499723 (488K) [binary/octet-stream]\n",
            "Saving to: ‘./tokenizer.model’\n",
            "\n",
            "./tokenizer.model   100%[===================>] 488.01K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-07-27 11:20:56 (67.4 MB/s) - ‘./tokenizer.model’ saved [499723/499723]\n",
            "\n",
            "--2023-07-27 11:20:56--  https://download.llamameta.net/tokenizer_checklist.chk?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiXHUwMDE1Kz8rP01fPyIsIlJlc291cmNlIjoiaHR0cHM6XC9cL2Rvd25sb2FkLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTA1NDIwMDd9fX1dfQ__&Signature=qUhzdbuQdJXmV63StYpm5YMpsZRI6Srz5-TL-uM9lE1JhBCy7G9JuW-TXAJi52CHDlgFQ3j-uwbuPBsQ8--kOcSXbXQc7Bt9lBv0lBunFp%7EmpGvVaa%7E%7Eop%7Exs45HIHpkvY2%7EmJoEI-NHOs51t0B57-C7Gi1qOCT3KsHM1gwaRk7ykcCOc2K%7E2yQvLWt2wxHBAQnFz7WNuuRc%7E8sIWAcrutksLa5H2%7EtJ6OBSCCusKMWUvh9Hi41oi-FasKGhzZ5VlnVJQfOSLJ5YjhdSmfc5qIsRg-LvYyCsabY43v0DursX2Tl01%7EpqkssDlfq6hrhg468-xK%7E-cla-f97%7E%7E6jf6w__&Key-Pair-Id=K15QRJLYKIFSLZ\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 13.33.88.72, 13.33.88.45, 13.33.88.62, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|13.33.88.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50 [binary/octet-stream]\n",
            "Saving to: ‘./tokenizer_checklist.chk’\n",
            "\n",
            "./tokenizer_checkli 100%[===================>]      50  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-27 11:20:57 (4.57 MB/s) - ‘./tokenizer_checklist.chk’ saved [50/50]\n",
            "\n",
            "tokenizer.model: OK\n",
            "Downloading llama-2-7b-chat\n",
            "--2023-07-27 11:20:57--  https://download.llamameta.net/llama-2-7b-chat/consolidated.00.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiXHUwMDE1Kz8rP01fPyIsIlJlc291cmNlIjoiaHR0cHM6XC9cL2Rvd25sb2FkLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTA1NDIwMDd9fX1dfQ__&Signature=qUhzdbuQdJXmV63StYpm5YMpsZRI6Srz5-TL-uM9lE1JhBCy7G9JuW-TXAJi52CHDlgFQ3j-uwbuPBsQ8--kOcSXbXQc7Bt9lBv0lBunFp%7EmpGvVaa%7E%7Eop%7Exs45HIHpkvY2%7EmJoEI-NHOs51t0B57-C7Gi1qOCT3KsHM1gwaRk7ykcCOc2K%7E2yQvLWt2wxHBAQnFz7WNuuRc%7E8sIWAcrutksLa5H2%7EtJ6OBSCCusKMWUvh9Hi41oi-FasKGhzZ5VlnVJQfOSLJ5YjhdSmfc5qIsRg-LvYyCsabY43v0DursX2Tl01%7EpqkssDlfq6hrhg468-xK%7E-cla-f97%7E%7E6jf6w__&Key-Pair-Id=K15QRJLYKIFSLZ\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 13.33.88.72, 13.33.88.45, 13.33.88.62, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|13.33.88.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13476925163 (13G) [binary/octet-stream]\n",
            "Saving to: ‘./llama-2-7b-chat/consolidated.00.pth’\n",
            "\n",
            "./llama-2-7b-chat/c 100%[===================>]  12.55G   239MB/s    in 60s     \n",
            "\n",
            "2023-07-27 11:21:58 (214 MB/s) - ‘./llama-2-7b-chat/consolidated.00.pth’ saved [13476925163/13476925163]\n",
            "\n",
            "--2023-07-27 11:21:58--  https://download.llamameta.net/llama-2-7b-chat/params.json?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiXHUwMDE1Kz8rP01fPyIsIlJlc291cmNlIjoiaHR0cHM6XC9cL2Rvd25sb2FkLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTA1NDIwMDd9fX1dfQ__&Signature=qUhzdbuQdJXmV63StYpm5YMpsZRI6Srz5-TL-uM9lE1JhBCy7G9JuW-TXAJi52CHDlgFQ3j-uwbuPBsQ8--kOcSXbXQc7Bt9lBv0lBunFp%7EmpGvVaa%7E%7Eop%7Exs45HIHpkvY2%7EmJoEI-NHOs51t0B57-C7Gi1qOCT3KsHM1gwaRk7ykcCOc2K%7E2yQvLWt2wxHBAQnFz7WNuuRc%7E8sIWAcrutksLa5H2%7EtJ6OBSCCusKMWUvh9Hi41oi-FasKGhzZ5VlnVJQfOSLJ5YjhdSmfc5qIsRg-LvYyCsabY43v0DursX2Tl01%7EpqkssDlfq6hrhg468-xK%7E-cla-f97%7E%7E6jf6w__&Key-Pair-Id=K15QRJLYKIFSLZ\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 13.33.88.62, 13.33.88.45, 13.33.88.113, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|13.33.88.62|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102 [application/json]\n",
            "Saving to: ‘./llama-2-7b-chat/params.json’\n",
            "\n",
            "./llama-2-7b-chat/p 100%[===================>]     102  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-27 11:21:58 (74.3 MB/s) - ‘./llama-2-7b-chat/params.json’ saved [102/102]\n",
            "\n",
            "--2023-07-27 11:21:58--  https://download.llamameta.net/llama-2-7b-chat/checklist.chk?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiXHUwMDE1Kz8rP01fPyIsIlJlc291cmNlIjoiaHR0cHM6XC9cL2Rvd25sb2FkLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTA1NDIwMDd9fX1dfQ__&Signature=qUhzdbuQdJXmV63StYpm5YMpsZRI6Srz5-TL-uM9lE1JhBCy7G9JuW-TXAJi52CHDlgFQ3j-uwbuPBsQ8--kOcSXbXQc7Bt9lBv0lBunFp%7EmpGvVaa%7E%7Eop%7Exs45HIHpkvY2%7EmJoEI-NHOs51t0B57-C7Gi1qOCT3KsHM1gwaRk7ykcCOc2K%7E2yQvLWt2wxHBAQnFz7WNuuRc%7E8sIWAcrutksLa5H2%7EtJ6OBSCCusKMWUvh9Hi41oi-FasKGhzZ5VlnVJQfOSLJ5YjhdSmfc5qIsRg-LvYyCsabY43v0DursX2Tl01%7EpqkssDlfq6hrhg468-xK%7E-cla-f97%7E%7E6jf6w__&Key-Pair-Id=K15QRJLYKIFSLZ\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 13.33.88.62, 13.33.88.45, 13.33.88.113, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|13.33.88.62|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 100 [binary/octet-stream]\n",
            "Saving to: ‘./llama-2-7b-chat/checklist.chk’\n",
            "\n",
            "./llama-2-7b-chat/c 100%[===================>]     100  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-27 11:21:59 (80.0 MB/s) - ‘./llama-2-7b-chat/checklist.chk’ saved [100/100]\n",
            "\n",
            "Checking checksums\n",
            "consolidated.00.pth: OK\n",
            "params.json: OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9cQsXliCUTu",
        "outputId": "26d5d652-0048-4900-c7f6-cd2205e5c08c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/llama\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from llama==0.0.1) (2.0.1+cu118)\n",
            "Collecting fairscale (from llama==0.0.1)\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fire (from llama==0.0.1)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece (from llama==0.0.1)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from fairscale->llama==0.0.1) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->llama==0.0.1) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->llama==0.0.1) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->llama==0.0.1) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->llama==0.0.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->llama==0.0.1) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->llama==0.0.1) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->llama==0.0.1) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->llama==0.0.1) (16.0.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llama==0.0.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llama==0.0.1) (2.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->llama==0.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->llama==0.0.1) (1.3.0)\n",
            "Building wheels for collected packages: fairscale, fire\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332112 sha256=d2bd0658f51a9d2ebddc5c2f22be7c59a6ba659acbe618f5ba34fac5efe58874\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=7529ae8dc06e510442ca51929a41dd3d63e84c4e49f1a8c0a0491d857d606510\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built fairscale fire\n",
            "Installing collected packages: sentencepiece, fire, fairscale, llama\n",
            "  Running setup.py develop for llama\n",
            "Successfully installed fairscale-0.4.13 fire-0.5.0 llama-0.0.1 sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!torchrun --nproc_per_node 1 example_chat_completion.py \\\n",
        "    --ckpt_dir llama-2-7b-chat/ \\\n",
        "    --tokenizer_path tokenizer.model \\\n",
        "    --max_seq_len 512 --max_batch_size 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8Jn9T2PKpUF",
        "outputId": "1349d51b-b677-46b4-ec0e-e377f262a9ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> initializing model parallel with size 1\n",
            "> initializing ddp with size 1\n",
            "> initializing pipeline with size 1\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 0 (pid: 7332) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 794, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "=====================================================\n",
            "example_chat_completion.py FAILED\n",
            "-----------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "-----------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-07-27_11:47:57\n",
            "  host      : 4501b31efbfd\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : -9 (pid: 7332)\n",
            "  error_file: <N/A>\n",
            "  traceback : Signal 9 (SIGKILL) received by PID 7332\n",
            "=====================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!torchrun --nproc_per_node 1 example_text_completion.py \\\n",
        "    --ckpt_dir llama-2-7b/ \\\n",
        "    --tokenizer_path tokenizer.model \\\n",
        "    --max_seq_len 128 --max_batch_size 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsnU5zqyETN8",
        "outputId": "7723fe0f-3643-4e50-8cf0-561561bf5e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/llama/example_text_completion.py\", line 55, in <module>\n",
            "    fire.Fire(main)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/content/llama/example_text_completion.py\", line 18, in main\n",
            "    generator = Llama.build(\n",
            "  File \"/content/llama/llama/generation.py\", line 62, in build\n",
            "    torch.distributed.init_process_group(\"nccl\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\", line 907, in init_process_group\n",
            "    default_pg = _new_process_group_helper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\", line 1024, in _new_process_group_helper\n",
            "    backend_class = ProcessGroupNCCL(backend_prefix_store, group_rank, group_size, pg_options)\n",
            "RuntimeError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 5545) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 794, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "example_text_completion.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-07-27_11:19:23\n",
            "  host      : 83497be86a4c\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 5545)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5GrVBSzEvp5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}